{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from jax import random\n",
    "\n",
    "from bayesian_active_learning.data_utils import NumpyDataset, NumpyLoader\n",
    "from bayesian_active_learning.utils import one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load + preprocess MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = torchvision.datasets.MNIST(\n",
    "    \"../datasets\", train=True, download=True\n",
    ")\n",
    "full_test_dataset = torchvision.datasets.MNIST(\n",
    "    \"../datasets\", train=False, download=True\n",
    ")\n",
    "\n",
    "num_classes = len(full_train_dataset.classes)\n",
    "total_train_samples = len(full_train_dataset.data)\n",
    "total_test_samples = len(full_train_dataset.data)\n",
    "\n",
    "all_train_X = np.array(full_train_dataset.data) / 255.0\n",
    "all_train_y = one_hot(np.array(full_train_dataset.targets), k=num_classes)\n",
    "\n",
    "all_test_X = np.array(full_test_dataset.data) / 255.0\n",
    "all_test_y = one_hot(np.array(full_test_dataset.targets), k=num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split train set into initial train set, validation set and pool set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_initial_train_points = 100\n",
    "num_validation_points = 100\n",
    "\n",
    "initial_train_X, val_X, inital_pool_X = np.split(\n",
    "    all_train_X,\n",
    "    [num_initial_train_points, num_initial_train_points + num_validation_points],\n",
    ")\n",
    "initial_train_y, val_y, inital_pool_y = np.split(\n",
    "    all_train_y,\n",
    "    [num_initial_train_points, num_initial_train_points + num_validation_points],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = NumpyLoader(\n",
    "    dataset=NumpyDataset(initial_train_X, initial_train_y), batch_size=16, shuffle=True\n",
    ")\n",
    "validation_generator = NumpyLoader(\n",
    "    dataset=NumpyDataset(val_X, val_y), batch_size=256, shuffle=True\n",
    ")\n",
    "test_generator = NumpyLoader(\n",
    "    dataset=NumpyDataset(all_test_X, all_test_y), batch_size=256\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "from jax import nn, random\n",
    "\n",
    "from bayesian_active_learning.models import BayesianConvNet\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "\n",
    "def forward(\n",
    "    num_classes: int,\n",
    "    dropout_rates: Tuple[float, float],\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    x: jnp.ndarray,\n",
    "):\n",
    "    net = BayesianConvNet(\n",
    "        num_classes=num_classes, dropout_rates=dropout_rates, activation=nn.relu\n",
    "    )\n",
    "\n",
    "    return net(x)\n",
    "\n",
    "\n",
    "def forward(\n",
    "    num_classes: int,\n",
    "    dropout_rates: Tuple[float, float],\n",
    "    activation: Callable[[jnp.ndarray], jnp.ndarray],\n",
    "    x: jnp.ndarray,\n",
    ") -> Callable[[jnp.ndarray], jnp.ndarray]:\n",
    "    net = BayesianConvNet(\n",
    "        num_classes=num_classes,\n",
    "        activation=activation,\n",
    "    )\n",
    "    return net(dropout_rates, x)\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "dropout_rates = (0.25, 0.5)\n",
    "activation = nn.relu\n",
    "\n",
    "model = partial(forward, num_classes, dropout_rates, activation)\n",
    "model = hk.transform(model)\n",
    "\n",
    "eval_model = partial(forward, num_classes, (0, 0), activation)\n",
    "eval_model = hk.without_apply_rng(hk.transform(eval_model))\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "params = model.init(key, jnp.zeros((1, 28, 28)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "def loss(\n",
    "    params: optax.Params, xs: jnp.ndarray, labels: jnp.ndarray, key\n",
    ") -> jnp.ndarray:\n",
    "    y_hat = model.apply(params, x=xs, rng=key)\n",
    "\n",
    "    # optax also provides a number of common loss functions.\n",
    "    loss_value = optax.softmax_cross_entropy(y_hat, labels)\n",
    "\n",
    "    return loss_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = jnp.ones((2, 28, 28))\n",
    "dummy_y = 1 / 10 * jnp.ones((2, 10))\n",
    "print(loss(params, dummy_x, dummy_y, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from bayesian_active_learning.metrics import compute_model_accuracy\n",
    "\n",
    "params = model.init(key, jnp.zeros((1, 28, 28)))\n",
    "optimizer = optax.adamw(1e-3, weight_decay=1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def step(params, optimizer_state, xs, labels, key):\n",
    "    grads = jax.grad(loss)(params, xs, labels, key)\n",
    "    updates, opt_state = optimizer.update(grads, optimizer_state, params)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "\n",
    "validation_accuracy_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "for epoch in trange(100):\n",
    "    for xs, labels in training_generator:\n",
    "        key, sub_key = random.split(key, 2)\n",
    "        params, opt_state = step(params, opt_state, xs, labels, sub_key)\n",
    "\n",
    "    # compute accuracy on validation and train set\n",
    "    train_accuracy = compute_model_accuracy(\n",
    "        partial(eval_model.apply, params), training_generator\n",
    "    )\n",
    "    validation_accuracy = compute_model_accuracy(\n",
    "        partial(eval_model.apply, params), validation_generator\n",
    "    )\n",
    "\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    validation_accuracy_history.append(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_accuracy_history)\n",
    "plt.plot(validation_accuracy_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Acquisition functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "\n",
    "\n",
    "def generate_logit_samples(\n",
    "    model: Callable[[jnp.ndarray, random.PRNGKeyArray], jnp.ndarray],\n",
    "    xs: jnp.ndarray,\n",
    "    num_samples: int,\n",
    "    key: random.PRNGKeyArray,\n",
    ") -> jnp.ndarray:\n",
    "    keys = random.split(key, num_samples)\n",
    "\n",
    "    return vmap(model, in_axes=(0, None))(keys, xs).transpose((1, 0, 2))\n",
    "\n",
    "\n",
    "def entropy(dist: jnp.ndarray) -> jnp.ndarray:\n",
    "    # expect batch * num_classes\n",
    "    return -jnp.sum(dist * jnp.log(dist), axis=-1)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def BALD(logit_samples: jnp.ndarray) -> jnp.ndarray:\n",
    "    # expect batch * num_samples * num_classes\n",
    "    probs = nn.softmax(logit_samples, axis=-1)\n",
    "\n",
    "    posterior_predictive = jnp.mean(probs, axis=1)\n",
    "\n",
    "    return entropy(posterior_predictive) - jnp.mean(entropy(probs), axis=1)\n",
    "\n",
    "\n",
    "def max_entropy(logit_samples: jnp.ndarray) -> jnp.ndarray:\n",
    "    # expect batch * num_samples * num_classes\n",
    "    probs = nn.softmax(logit_samples, axis=-1)\n",
    "\n",
    "    posterior_predictive = jnp.mean(probs, axis=1)\n",
    "\n",
    "    return entropy(posterior_predictive)\n",
    "\n",
    "\n",
    "def random(logit_samples: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.ones(logit_samples.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_model = partial(model.apply, params)\n",
    "\n",
    "samples = generate_logit_samples(predictive_model, jnp.ones((2, 28, 28)), 2, key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb4a0ac80907d7f44e1a5e88d3d3381b33e3dbedd3a24d113e876f30a0c46bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
