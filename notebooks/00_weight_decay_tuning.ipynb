{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from jax import random\n",
    "\n",
    "from bayesian_active_learning.acquisition_functions import (\n",
    "    BALD,\n",
    "    max_entropy,\n",
    "    uniform,\n",
    ")\n",
    "from bayesian_active_learning.data_utils import NumpyDataset, NumpyLoader\n",
    "from bayesian_active_learning.experiment import experiment_run\n",
    "from bayesian_active_learning.utils import one_hot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook does a scan over the weight decay hyperparameter, to determine the appropriate value to maximise the validation accuracy. N.B We assume that the weight decay is tuned for the validation set for the first batch of training data only, and not on every subsequent receipt of data from the pool set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load + preprocess MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = torchvision.datasets.MNIST(\n",
    "    \"../datasets\", train=True, download=True\n",
    ")\n",
    "\n",
    "num_classes = len(full_train_dataset.classes)\n",
    "total_train_samples = len(full_train_dataset.data)\n",
    "total_test_samples = len(full_train_dataset.data)\n",
    "\n",
    "all_train_X = np.array(full_train_dataset.data) / 255.0\n",
    "all_train_y = one_hot(np.array(full_train_dataset.targets), k=num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split train set into initial train set, validation set and pool set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_initial_train_points = 100\n",
    "num_validation_points = 100\n",
    "\n",
    "initial_train_X, val_X, initial_pool_X = np.split(\n",
    "    all_train_X,\n",
    "    [num_initial_train_points, num_initial_train_points + num_validation_points],\n",
    ")\n",
    "initial_train_y, val_y, initial_pool_y = np.split(\n",
    "    all_train_y,\n",
    "    [num_initial_train_points, num_initial_train_points + num_validation_points],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = NumpyLoader(\n",
    "    dataset=NumpyDataset(initial_train_X, initial_train_y), batch_size=16, shuffle=True\n",
    ")\n",
    "validation_generator = NumpyLoader(dataset=NumpyDataset(val_X, val_y), batch_size=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. tuning the weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from bayesian_active_learning.losses import classification_loss\n",
    "from bayesian_active_learning.models import model\n",
    "from bayesian_active_learning.training import fit\n",
    "\n",
    "# create, transform and intialise model (and evaluation model)\n",
    "num_classes = 10\n",
    "dropout_rates = (0.25, 0.5)\n",
    "\n",
    "base_training_model = partial(model, num_classes, dropout_rates)\n",
    "stochastic_model = hk.transform(base_training_model)\n",
    "\n",
    "base_eval_model = partial(model, num_classes, (0, 0))\n",
    "eval_model = hk.without_apply_rng(hk.transform(base_eval_model))\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "params = stochastic_model.init(subkey, jnp.zeros((1, 28, 28)))\n",
    "\n",
    "loss = partial(classification_loss, stochastic_model)\n",
    "\n",
    "# train the model using the initial training data\n",
    "for weight_decay in jnp.logspace(-3, -1.5, 5):\n",
    "    optimiser = optax.adamw(1e-3, weight_decay=weight_decay)\n",
    "\n",
    "    params, metrics = fit(\n",
    "        loss=loss,\n",
    "        params=params,\n",
    "        eval_model=eval_model,\n",
    "        optimiser=optimiser,\n",
    "        num_epochs=100,\n",
    "        train_generator=training_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        key=key,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"weight decay; {weight_decay}, validation accuracy: {jnp.mean(metrics.validation_accuracy_history[:-20])}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb4a0ac80907d7f44e1a5e88d3d3381b33e3dbedd3a24d113e876f30a0c46bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
